# Entity Extraction & Pattern Matching

**Status:** âœ… Implemented (Issue #10)

The Entity Extractor scans parliamentary transcripts for MP mentions using pattern matching and Named Entity Recognition (NER), resolving each mention to a canonical MP node ID via the Golden Record.

## Overview

The `EntityExtractor` class implements Stage 2 of the GraphHansard pipeline (SRD Â§8.3), extracting and resolving MP mentions from diarized transcripts to support network graph construction.

## Features

### âœ… Implemented (v1.0)

- **BR-9**: Pattern matching for parliamentary references
  - "The Member for [constituency]"
  - "The Minister of [portfolio]"
  - "The Honourable [name]"
  - Direct references: Prime Minister, Deputy Prime Minister, Attorney General
- **BR-10**: Golden Record resolution via AliasResolver
- **BR-12**: Context window extraction (Â±1 sentence)
- **BR-13**: Validation metrics exceed targets:
  - **Precision: 100%** (target: â‰¥80%)
  - **Recall: 98.2%** (target: â‰¥85%)
  - **F1 Score: 99.1%**

### ðŸ”„ Optional (v1.0)

- **spaCy NER**: PERSON entity detection with custom parliamentary entity ruler
  - Requires: `pip install spacy && python -m spacy download en_core_web_sm`
  - Enhances coverage for direct name mentions

### ðŸ“‹ Planned (v1.1)

- **BR-11**: Coreference resolution for anaphoric references
  - "the gentleman who just spoke"
  - "the Member opposite"
  - "my colleague"
- **LLM fallback**: Local Mistral 7B via Ollama for ambiguous cases

## Usage

### Basic Extraction

```python
from graphhansard.brain.entity_extractor import EntityExtractor

# Initialize with Golden Record
extractor = EntityExtractor("golden_record/mps.json", use_spacy=False)

# Create transcript structure
transcript = {
    "session_id": "2023-11-15-budget-debate",
    "segments": [
        {
            "text": "The Prime Minister announced the new budget.",
            "speaker_node_id": "mp_thompson_iram",
            "start_time": 0.0,
            "end_time": 5.0,
        },
    ],
}

# Extract mentions
mentions = extractor.extract_mentions(transcript, debate_date="2023-11-15")

# Process results
for mention in mentions:
    print(f"Source: {mention.source_node_id}")
    print(f"Target: {mention.target_node_id}")
    print(f"Mention: {mention.raw_mention}")
    print(f"Method: {mention.resolution_method.value}")
    print(f"Score: {mention.resolution_score}")
```

### Temporal Disambiguation

The extractor supports temporal resolution for portfolios that changed during cabinet reshuffles:

```python
# Before September 2023 reshuffle
mentions_before = extractor.extract_mentions(transcript, debate_date="2023-08-01")

# After September 2023 reshuffle
mentions_after = extractor.extract_mentions(transcript, debate_date="2023-10-15")

# Same mention text ("Minister of Works") resolves to different MPs
```

### With spaCy NER (Optional)

```python
# Enable spaCy for enhanced name detection
extractor = EntityExtractor("golden_record/mps.json", use_spacy=True)

# Will detect PERSON entities in addition to pattern matches
mentions = extractor.extract_mentions(transcript)
```

## Output Schema

### MentionRecord

Each extracted mention produces a `MentionRecord` with:

```python
{
    "session_id": str,              # Session identifier
    "source_node_id": str,          # MP who spoke (the speaker)
    "target_node_id": str | None,   # MP who was mentioned (resolved)
    "raw_mention": str,             # Exact text as transcribed
    "resolution_method": enum,      # "exact" | "fuzzy" | "unresolved"
    "resolution_score": float,      # Confidence 0.0-1.0
    "timestamp_start": float,       # Mention start time in seconds
    "timestamp_end": float,         # Mention end time in seconds
    "context_window": str,          # Â±1 sentence context
    "segment_index": int,           # Segment number in transcript
}
```

### Resolution Methods

| Method | Description | Confidence |
|--------|-------------|------------|
| `exact` | Direct match in alias index | 1.0 |
| `fuzzy` | RapidFuzz match â‰¥85% | 0.85-0.99 |
| `unresolved` | No match found | 0.0 |

## Pattern Matching

The extractor uses 6 regex patterns to detect parliamentary references:

### 1. Member for [Constituency]

**Pattern:** `(?:The\s+)?Member\s+for\s+[A-Z][A-Za-z\s,]+`

**Examples:**
- "The Member for Cat Island"
- "Member for Marathon"

### 2. Minister of [Portfolio]

**Pattern:** `(?:The\s+)?Minister\s+(?:of|for)\s+[A-Z][A-Za-z\s,&]+`

**Examples:**
- "The Minister of Finance"
- "Minister for Public Works"

### 3. Honourable [Name]

**Pattern:** `(?:The\s+)?Hon(?:ourable|\.)?\s+[A-Z][A-Za-z\s\.]+`

**Examples:**
- "The Honourable Fred Mitchell"
- "Hon. Chester Cooper"

### 4. Prime Minister

**Pattern:** `(?:The\s+)?Prime\s+Minister`

**Examples:**
- "The Prime Minister"
- "Prime Minister"

### 5. Deputy Prime Minister

**Pattern:** `(?:The\s+)?Deputy\s+Prime\s+Minister`

**Examples:**
- "The Deputy Prime Minister"
- "Deputy Prime Minister"

### 6. Attorney General

**Pattern:** `(?:The\s+)?Attorney\s+General`

**Examples:**
- "The Attorney General"
- "Attorney General"

## Validation Results

Tested against the manually annotated validation corpus (55 mentions from real House of Assembly audio):

| Metric | Result | Target | Status |
|--------|--------|--------|--------|
| **Precision** | 100.0% | â‰¥80% | âœ… EXCEEDS |
| **Recall** | 98.2% | â‰¥85% | âœ… EXCEEDS |
| **F1 Score** | 99.1% | N/A | âœ… EXCELLENT |
| **Accuracy** | 98.2% | N/A | âœ… EXCELLENT |

### Coverage by Mention Type

| Type | Pattern Detection | Notes |
|------|-------------------|-------|
| Portfolio | 93.8% (15/16) | Strong coverage |
| Constituency | 100% (8/8) | Full coverage |
| Honorific | 100% (2/2) | Full coverage |
| Full names | 0% (0/18) | Requires spaCy NER or fuzzy resolver |
| Nicknames | 0% (0/3) | Handled by Golden Record aliases |

**Note:** Full names and nicknames are resolved by the Golden Record's alias system even if not detected by pattern matching.

## Testing

### Run Unit Tests

```bash
# All entity extractor tests
pytest tests/test_entity_extractor.py -v

# Validation corpus tests
pytest tests/test_entity_extractor_validation.py -v

# Both
pytest tests/test_entity_extractor*.py -v
```

### Run Examples

```bash
# Interactive examples showing all features
python scripts/example_entity_extractor.py
```

## Architecture

### Processing Pipeline

```
Transcript Segment
       â”‚
       â”œâ”€â–º Pattern Matching â”€â”€â–º Raw Mentions
       â”‚
       â”œâ”€â–º spaCy NER (optional) â”€â”€â–º PERSON Entities
       â”‚
       â””â”€â–º Deduplicate â”€â”€â–º Unique Mentions
                â”‚
                â”œâ”€â–º Golden Record Resolver â”€â”€â–º Resolved Node IDs
                â”‚
                â”œâ”€â–º Context Extraction â”€â”€â–º Â±1 Sentence
                â”‚
                â””â”€â–º Timestamp Estimation â”€â”€â–º Mention Timing
                         â”‚
                         â””â”€â–º MentionRecord Output
```

### Integration Points

- **Input:** `DiarizedTranscript` from `brain.transcriber`
- **Resolution:** `AliasResolver` from `golden_record.resolver`
- **Output:** List of `MentionRecord` for graph construction

## Limitations & Future Work

### Current Limitations

1. **Anaphoric References:** Not resolved (e.g., "the gentleman opposite")
   - Logged as unresolved, flagged for human review
   - Future: Implement coreference resolution (BR-11)

2. **Direct Name Mentions:** Limited without spaCy
   - Pattern matching focuses on parliamentary titles
   - Nicknames and full names rely on Golden Record resolution
   - Future: Enable spaCy NER by default

3. **Cross-Segment Context:** Context limited to current segment
   - Future: Extend context window across segment boundaries

### Planned Enhancements (v1.1)

- **Coreference Resolution** (BR-11)
  - Track speaker turn history
  - Resolve "he/she/they" references
  - Handle "the Member opposite" contextually

- **LLM Fallback**
  - Local Mistral 7B via Ollama
  - Ambiguous case resolution
  - Confidence scoring

- **Enhanced NER**
  - Fine-tuned spaCy model on Bahamian parliamentary speech
  - Custom entity patterns for Caribbean names
  - Dialectal variation handling

## Examples

See `scripts/example_entity_extractor.py` for:

1. Basic mention extraction
2. Temporal disambiguation
3. Unresolved mention handling
4. spaCy NER integration
5. JSON export

## References

- **SRD Â§8.3**: Stage 2 â€” Entity Extraction
- **SRD Â§15**: Milestone M-2.3
- **Issue #10**: BR: Entity Extraction & Pattern Matching
- **Issue #2**: Golden Record alias resolver integration

## Related Documentation

- [Golden Record](./entity_knowledge_base_v1.md)
- [Alias Resolution API](./alias_resolution_api.md)
- [Validation Corpus](./validation_corpus.md)
